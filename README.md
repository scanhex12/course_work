# Graph DL for recsys

## Week 0

Смотрим на различные функции потерь на игрушечных данных(REDDIT-BINARY)

Baseline: Node2Vec(map@k = 5.0551098452926256e-05)

Лоссы:

BPR: (map@k = 0.02576476279172048)

BCE: (map@k = 0.0090071146505755)

Focal: (map@k = 0.00020740376155565485)

Модификация MarginRankingLoss: (map@k = 0.00022151056022957357)

## Week 1

Смотрим на данные VK Cup, делаем EDA, пробуем описанные выше методы(опционально)

### TLDR EDA

1. Средняя степень вершины 10, максимальная 6000. Распеределение степеней вершин можно посмотреть в ноутбуке
2. Всего компонент связности 31188. Большая часть из них (99.5%) имеют не более 5 вершин. Есть одна большая компонента на 3166590 вершин(98% от общего числа вершин)
3. Если брать центральности как собственный вектор, соответствующий максимальному собственному значению, то распеределение центральностей похоже на нормальное, однако имеет второй "горб"
4. В среднем с увеличением центральности возрастает степень вершины.
5. В среднем кластаризуемость падает с увеличением степени вершины
6. h, t распеределены равномерно и независимо от степени, центральности и кластеризуемости вершин.

БОльшая компонента графа состоит из довольно плотных подграфов, как-то соединенными между собой. 

### Сравнение GCN и случайного подбрасывания монетки 

TODO


## Week 2

TODO:
- Посмотреть валидационный датасет
- Запустить инференс на датасете с разыми гипотезами: бфс, прочие предположения(подумоть)
- Обучить сетку, посмотреть на обучение сеток поменьше
- Написать PageRank

Написать результаты

## Week 3

Используем подходы из metric learning - обучаем на triplet loss.

Замечаем, что все вектора "схлопываются" в одну точку, то есть опять же, сетка ничего не учит


## Week 4

Смотрим, можно ли улучшить подходы SLIM, ALS, implicit ALS, SVD++ с помощью gnn

Делаем 2 сценария: 
1. В качестве эмбеддингов кладем полученные вектора и замораживаем слой эмбеддинга
2. В качестве эмбеддингов кладем полученные вектора и дообучаем их с очень малым lr

Статьи:

SVD++: https://arxiv.org/abs/2203.11026

## Week 5

Смотрим на новую задачу - рекоммендация для "группы", датасет - OTTO

Суть задачи - дана какая-то группа предметов, которые упорядоченны по времени, нужно дополнить(порекоммендовать другие предметы в эту группу) ее до нужного размера. Задача очень похожа на sequential recommendations

Baseline - word2vec

Смотрим можем ли улучшить его с помощью графовых нейронных сетей

Архитектура остается такой же, однако меняем лосс на аналогичный word2vec, чтобы сравнение было более честным

## Week 6

На датасете OTTO смотрим начинаем смотреть на различные архитектуры, пытаемся улучшить их графовыми сетями

SHAN, статья: https://www.ijcai.org/proceedings/2018/0546.pdf

## Week 7+

Дописать (произошло очень много чего)
